{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T13:33:54.238706Z",
     "start_time": "2020-06-13T13:33:54.225737Z"
    }
   },
   "source": [
    "<b> If you are just starting your Data Science journey or would like to learn about some cool \n",
    "python libraries for data science, then I recommend you to check out this-\n",
    "<a href='https://medium.com/swlh/start-your-data-science-journey-today-37366ee463f'> Start your Data Science journey today </a>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:25.717962Z",
     "start_time": "2020-06-20T09:18:23.141363Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml                        # using openml to import dataset \n",
    "from pandas_profiling import ProfileReport                       \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer                   # to transform column of different types\n",
    "from sklearn.model_selection import train_test_split            \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV                # to find best hyper parameters\n",
    "\n",
    "# import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# To display multiple output from a single cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing TITANIC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TITANIC dataset contains the details of the passengers who were onboard on Titanic ship and whether they survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:25.781644Z",
     "start_time": "2020-06-20T09:18:25.720019Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing TITANIC dataset using openml, more details at- https://www.openml.org/d/40945\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many columns which needs proper analysis like Name, ticket. As here the main idea is to just show how we can create data pipeline. We will use the columns which require minimium preprocessing i.e. (Age, Sex, PClass, Fare, Embarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:25.802102Z",
     "start_time": "2020-06-20T09:18:25.783771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass                            name     sex      age  sibsp  parch  \\\n",
       "0     1.0   Allen, Miss. Elisabeth Walton  female  29.0000    0.0    0.0   \n",
       "1     1.0  Allison, Master. Hudson Trevor    male   0.9167    1.0    2.0   \n",
       "\n",
       "   ticket      fare    cabin embarked boat  body  \\\n",
       "0   24160  211.3375       B5        S    2   NaN   \n",
       "1  113781  151.5500  C22 C26        S   11   NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "Name: survived, dtype: category\n",
       "Categories (2, object): [0, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top rows of the dataset\n",
    "X.head(2)\n",
    "y.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling Report using Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:25.837584Z",
     "start_time": "2020-06-20T09:18:25.803889Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating a profile report using pandas profiling\n",
    "combine_dataset = pd.concat([X, y], axis=1)         # combine dataset to profile it together\n",
    "profile_report = ProfileReport(combine_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the report, there are missing values for different columns and we need to impute these values based on different strategy. For this demo, we have used SimpleImputer and grid searched for best strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:43.733142Z",
     "start_time": "2020-06-20T09:18:25.839376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734c4c76f1db475a995e0671f84fac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Summarize dataset', max=28.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee374e2533024c14a9fa8122e413f0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generate report structure', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render widgets', max=1.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2492d00ae0d7467c898757a971e03f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile_report.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.904429Z",
     "start_time": "2020-06-20T09:18:43.736884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb06ab2168c4f6d9b59847a25bd8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Render HTML', max=1.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923309f3ed244beeb7bd4cd6969e5c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Export report to file', max=1.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# saving profile report\n",
    "profile_report.to_file('titanic_profiling_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Operations on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.909938Z",
     "start_time": "2020-06-20T09:18:44.907215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical features in the data\n",
    "numeric_features = ['age', 'fare']\n",
    "\n",
    "# categorical features in the data\n",
    "categorical_features = ['embarked', 'sex', 'pclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we need to tackle missing values as many models can not handle them. Sklearn provides different strategies, below we will see how we can use SimpleImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.915913Z",
     "start_time": "2020-06-20T09:18:44.912267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets make a copy of X\n",
    "X_copy = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.920988Z",
     "start_time": "2020-06-20T09:18:44.918221Z"
    }
   },
   "outputs": [],
   "source": [
    "# As we can see from the report, Age and embarked have few missing values. \n",
    "# So, we need to impute these values before analyzing\n",
    "\n",
    "# Lets see how imputer works and then we will see how we can create this using pipeline\n",
    "# All the missing values will be filled with mean\n",
    "impute_age = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.934555Z",
     "start_time": "2020-06-20T09:18:44.923308Z"
    }
   },
   "outputs": [],
   "source": [
    "X_copy['age_imputed'] = impute_age.fit_transform(X[['age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.962016Z",
     "start_time": "2020-06-20T09:18:44.937044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "      <th>age_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Baumann, Mr. John D</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PC 17318</td>\n",
       "      <td>25.925</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>29.881135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Bradley, Mr. George ('George Arthur Brayton')</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111427</td>\n",
       "      <td>26.550</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>29.881135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pclass                                           name   sex  age  sibsp  \\\n",
       "15     1.0                            Baumann, Mr. John D  male  NaN    0.0   \n",
       "37     1.0  Bradley, Mr. George ('George Arthur Brayton')  male  NaN    0.0   \n",
       "\n",
       "    parch    ticket    fare cabin embarked  boat  body        home.dest  \\\n",
       "15    0.0  PC 17318  25.925  None        S  None   NaN     New York, NY   \n",
       "37    0.0    111427  26.550  None        S     9   NaN  Los Angeles, CA   \n",
       "\n",
       "    age_imputed  \n",
       "15    29.881135  \n",
       "37    29.881135  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how missing values are filled\n",
    "X_copy[X_copy['age'].isna()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.967763Z",
     "start_time": "2020-06-20T09:18:44.964304Z"
    }
   },
   "outputs": [],
   "source": [
    "# embarked contains string values and hence we can not use mean or median.\n",
    "# Two strategy which is already defined for string data is constant or most_frequent\n",
    "\n",
    "# Below we used most_frequent which means the embarked port which had most passengers will be used\n",
    "impute_embarked = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:44.980187Z",
     "start_time": "2020-06-20T09:18:44.970471Z"
    }
   },
   "outputs": [],
   "source": [
    "X_copy['embarked_imputed'] = impute_embarked.fit_transform(X[['embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.004157Z",
     "start_time": "2020-06-20T09:18:44.982635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "      <th>age_imputed</th>\n",
       "      <th>embarked_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Icard, Miss. Amelie</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>38.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cincinatti, OH</td>\n",
       "      <td>62.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass                                       name     sex   age  sibsp  \\\n",
       "168     1.0                        Icard, Miss. Amelie  female  38.0    0.0   \n",
       "284     1.0  Stone, Mrs. George Nelson (Martha Evelyn)  female  62.0    0.0   \n",
       "\n",
       "     parch  ticket  fare cabin embarked boat  body       home.dest  \\\n",
       "168    0.0  113572  80.0   B28      NaN    6   NaN            None   \n",
       "284    0.0  113572  80.0   B28      NaN    6   NaN  Cincinatti, OH   \n",
       "\n",
       "     age_imputed embarked_imputed  \n",
       "168         38.0                S  \n",
       "284         62.0                S  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how missing values are filled\n",
    "\n",
    "# S is indeed the most frequent port if you see in profile report\n",
    "X_copy[X_copy['embarked'].isna()].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning models need to have data with in a scale as otherwise they can get affected by different variance of different features. Below we will see how we can apply scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.009697Z",
     "start_time": "2020-06-20T09:18:45.007175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets see how scaling can help on 1 column\n",
    "\n",
    "# this scale will scale the values within the range [0,1]\n",
    "minmax_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.020373Z",
     "start_time": "2020-06-20T09:18:45.012596Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can apply scaler on multiple columns which we do below with the pipeling.\n",
    "# This is only to show the effect of scaler\n",
    "X_copy['age_scaled'] = minmax_scaler.fit_transform(X[['age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.029246Z",
     "start_time": "2020-06-20T09:18:45.023423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the values are scaled between [0,1]\n",
    "X_copy['age_scaled'].min()\n",
    "X_copy['age_scaled'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, when we have a feature which can have only certain range of values i.e. Embarked feature can have only 3 values, we apply one hot encoding to basically convert each column into a number of columns which is equal to the number of distinct values for the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.039879Z",
     "start_time": "2020-06-20T09:18:45.037214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets for example see how applyin One hot encoding on embarked feature works\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.050447Z",
     "start_time": "2020-06-20T09:18:45.043053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See below three different columns are created\n",
    "one_hot_encoder.fit_transform(X_copy[['embarked_imputed']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.055368Z",
     "start_time": "2020-06-20T09:18:45.051881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_C', 'x0_Q', 'x0_S'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features for each value of embarked\n",
    "one_hot_encoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know what each operation do. We will now see how we can utilize Data pipeline directly to apply all these operations.\n",
    "\n",
    "Data Pipeline also helps in <b>data leakage problem</b> as while applying various operations we applied it on the entire dataset and hence we had given some visibility into the test dataset. Although, there are other ways to overcome that, we will use data pipelines which is much cleaner approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.059010Z",
     "start_time": "2020-06-20T09:18:45.056849Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.063868Z",
     "start_time": "2020-06-20T09:18:45.060656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a pipeline which can be used on numerical data.\n",
    "\n",
    "# This is sequential. So, we are first imputing the missing values \n",
    "# and then applying MinMaxScaler\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', 'passthrough')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.067702Z",
     "start_time": "2020-06-20T09:18:45.065517Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['embarked', 'sex', 'pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.072413Z",
     "start_time": "2020-06-20T09:18:45.069565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a pipeline which can be used on categorical data.\n",
    "\n",
    "# This is sequential. So, we are first imputing the missing values \n",
    "# and then applying OneHotEncoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.076168Z",
     "start_time": "2020-06-20T09:18:45.073820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit learn provides ColumnTransformer which can be used to apply\n",
    "# different transformer(operations) to different set of columns\n",
    "\n",
    "# Below we are applying our numerical_transformer and categorical_transform for our\n",
    "# numerical and categorical features\n",
    "data_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numerical_transformer, numerical_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.080922Z",
     "start_time": "2020-06-20T09:18:45.078258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets add PCA to the Pipeline\n",
    "# PCA might not be needed here as it is a small dataset\n",
    "# we are adding it only for demo purposes.\n",
    "\n",
    "preprocessor = Pipeline(steps=[('data_transformer', data_transformer),\n",
    "                             ('reduce_dim',PCA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.084992Z",
     "start_time": "2020-06-20T09:18:45.082419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets append classifer to our data transformer\n",
    "\n",
    "# we are using Logistics Regression here\n",
    "classifier = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(random_state=0, max_iter=10000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.090718Z",
     "start_time": "2020-06-20T09:18:45.087292Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can utilize params grid to check for best hyperparameters or transfomers\n",
    "\n",
    "# The syntax here is transformer_name__parameters and we need to chain if we have nested pipelines \n",
    "param_grid = {\n",
    "    'preprocessor__data_transformer__numerical__imputer__strategy': ['mean', 'median'],\n",
    "    'preprocessor__data_transformer__categorical__imputer__strategy': ['constant','most_frequent'],\n",
    "    'preprocessor__data_transformer__numerical__scaler': [StandardScaler(), RobustScaler(), \\\n",
    "                                                          MinMaxScaler()],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "    'preprocessor__reduce_dim__n_components': [2, 5, 10],\n",
    "    'classifier__solver': ['liblinear','newton-cg', 'lbfgs','sag','saga']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.102221Z",
     "start_time": "2020-06-20T09:18:45.092323Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:18:45.106442Z",
     "start_time": "2020-06-20T09:18:45.104087Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(classifier, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.193Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Sklearn 0.23.1, we can even visualize our estimator using the set_config option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.195Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.196Z"
    }
   },
   "outputs": [],
   "source": [
    "# set config to diagram for visualizing the pipelines\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets see our best estimator\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.201Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving pipeling as html format\n",
    "from sklearn.utils import estimator_html_repr\n",
    "with open('titanic_data_pipeline_estimator.html', 'w') as f:  \n",
    "    f.write(estimator_html_repr(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.203Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.205Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.207Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(grid_search, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-20T09:18:23.209Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-env",
   "language": "python",
   "name": "demo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
